import re
import sys
import math
import socket
import urllib
from math import log2
from urllib.parse import urlparse
from lib.utils.extract_tld import ExtractTLD

def getRootDomain(url):
    root_domain = ExtractTLD(urlparse(url).hostname) if "://" in url else ExtractTLD(url) 
    return root_domain

def isIP(ip):
    return all([_.isdigit() for _ in ip.split(".")]) if ip.count(".")==3 else False

def isCDN(domain):
    if len(set( [ _[4] for _ in socket.getaddrinfo(domain,"http") ] )) >= 2 :
        return socket.getaddrinfo("www.4399.com", "http")[0][4][0]
    else:
        return False
def RequestProcessor(uri,data=None):
    if not uri:
        return dict()
    uri=uri if uri.startswith("http://") or uri.startswith("https://") else "http://"+uri
    _parser_=urlparse(uri)

    #{ "webroot":"http://xxx.com",  "path":"/xxx/xxx",  "params" :{id:[1],a:[2]},  "data"   :"xxxxx" }

    return {"webroot":_parser_.scheme+"://"+_parser_.netloc,
            "path":_parser_.path if _parser_.path else "/",
            "params":urllib.parse.parse_qs(_parser_.query),
            "data":data}

def Parameter(para):
    if para in sys.argv:
        para=sys.argv.index(para)
        para=sys.argv[para+1] if len(sys.argv) > para+1 else None
        return para

def GetTitle(text):
    match = re.search(r'<title.*?>(.+?)</title>', text)
    if match: return match.group(1)
    else: return str()

# progress_bar function to print ProgressBar ( Attention: Must print()! )
def progress_bar(title, iterable, bar_length=50):
    total_length = len(iterable)
    
    for index, item in enumerate(iterable):
        percent = (index + 1) / total_length

        arrow = 'â–†' * int(round(percent * bar_length) - 1)
        spaces = ' ' * (bar_length - len(arrow))

        sys.stdout.write(f'\r  [{title}] [{arrow + spaces}] {percent * 100:.2f}%')
        sys.stdout.flush()

        yield item

def Program(tasks:list, num_chunks:int, params=False) -> list:
    cache=list()
    if params==True:
        for task in tasks:
            for _ in cache:
                if task["webroot"]==_["webroot"] and task["path"]==_["path"]:
                    if task.get("params") and (set(task["params"])<=set(_["params"])):
                        break
            cache.append(task)
        tasks=cache
    if not tasks: return list()
    task_size=math.ceil( len(tasks)/num_chunks )
    return [ tasks[_:_+task_size] for _ in range(int() ,len(tasks) ,task_size) ]

def Entropy(data):
    total_count = len(data)
    unique_values = set(data)
    probabilities = [data.count(value) / total_count for value in unique_values]
    entropy = -sum(p * log2(p) for p in probabilities)
    return entropy

def CoefficientOfVariation(data):
    mean = sum(data) / len(data)
    if mean==0:
        return 0
    variance = sum((x - mean) ** 2 for x in data) / len(data)
    std_deviation = variance ** 0.5
    coefficient_of_variation = std_deviation / mean
    return coefficient_of_variation

def distance_euclidean(instance1, instance2):
    """Computes the distance between two instances. Instances should be tuples of equal length.
    Returns: Euclidean distance
    Signature: ((attr_1_1, attr_1_2, ...), (attr_2_1, attr_2_2, ...)) -> float"""

    def detect_value_type(attribute):
        """Detects the value type (number or non-number).
        Returns: (value type, value casted as detected type)
        Signature: value -> (str or float type, str or float value)"""
        from numbers import Number
        attribute_type = None
        if isinstance(attribute, Number):
            attribute_type = float
            attribute = float(attribute)
        else:
            attribute_type = str
            attribute = str(attribute)
        return attribute_type, attribute

    # check if instances are of same length
    if len(instance1) != len(instance2):
        raise AttributeError("Instances have different number of arguments.")
    # init differences vector
    differences = [0] * len(instance1)
    # compute difference for each attribute and store it to differences vector
    for i, (attr1, attr2) in enumerate(zip(instance1, instance2)):
        type1, attr1 = detect_value_type(attr1)
        type2, attr2 = detect_value_type(attr2)
        # raise error is attributes are not of same data type.
        if type1 != type2:
            raise AttributeError("Instances have different data types.")
        if type1 is float:
            # compute difference for float
            differences[i] = attr1 - attr2
        else:
            # compute difference for string
            if attr1 == attr2:
                differences[i] = 0
            else:
                differences[i] = 1
    # compute RMSE (root mean squared error)
    rmse = (sum(map(lambda x: x ** 2, differences)) / len(differences)) ** 0.5
    return rmse


class LOF:
    """Helper class for performing LOF computations and instances normalization."""

    def __init__(self, instances, normalize=True, distance_function=distance_euclidean):
        self.instances = instances
        self.normalize = normalize
        self.distance_function = distance_function
        if normalize:
            self.normalize_instances()

    def compute_instance_attribute_bounds(self):
        min_values = [float("inf")] * len(self.instances[0])  # n.ones(len(self.instances[0])) * n.inf
        max_values = [float("-inf")] * len(self.instances[0])  # n.ones(len(self.instances[0])) * -1 * n.inf
        for instance in self.instances:
            min_values = tuple(map(lambda x, y: min(x, y), min_values, instance))  # n.minimum(min_values, instance)
            max_values = tuple(map(lambda x, y: max(x, y), max_values, instance))  # n.maximum(max_values, instance)
        self.max_attribute_values = max_values
        self.min_attribute_values = min_values

    def normalize_instances(self):
        """Normalizes the instances and stores the infromation for rescaling new instances."""
        if not hasattr(self, "max_attribute_values"):
            self.compute_instance_attribute_bounds()
        new_instances = []
        for instance in self.instances:
            new_instances.append(
                self.normalize_instance(instance))  # (instance - min_values) / (max_values - min_values)
        self.instances = new_instances

    def normalize_instance(self, instance):
        return tuple(map(lambda value, max, min: (value - min) / (max - min) if max - min > 0 else 0,
                         instance, self.max_attribute_values, self.min_attribute_values))

    def local_outlier_factor(self, min_pts, instance):
        """The (local) outlier factor of instance captures the degree to which we call instance an outlier.
        min_pts is a parameter that is specifying a minimum number of instances to consider for computing LOF value.
        Returns: local outlier factor
        Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float"""
        if self.normalize:
            instance = self.normalize_instance(instance)
        return local_outlier_factor(min_pts, instance, self.instances, distance_function=self.distance_function)


def k_distance(k, instance, instances, distance_function=distance_euclidean):
    # TODO: implement caching
    """Computes the k-distance of instance as defined in paper. It also gatheres the set of k-distance neighbours.
    Returns: (k-distance, k-distance neighbours)
    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> (float, ((attr_j_1, ...),(attr_k_1, ...), ...))"""
    distances = {}
    for instance2 in instances:
        distance_value = distance_function(instance, instance2)
        if distance_value in distances:
            distances[distance_value].append(instance2)
        else:
            distances[distance_value] = [instance2]
    distances = sorted(distances.items())
    neighbours = []
    k_sero = 0
    k_dist = None
    for dist in distances:
        k_sero += len(dist[1])
        neighbours.extend(dist[1])
        k_dist = dist[0]
        if k_sero >= k:
            break
    return k_dist, neighbours


def reachability_distance(k, instance1, instance2, instances, distance_function=distance_euclidean):
    """The reachability distance of instance1 with respect to instance2.
    Returns: reachability distance
    Signature: (int, (attr_1_1, ...),(attr_2_1, ...)) -> float"""
    (k_distance_value, neighbours) = k_distance(k, instance2, instances, distance_function=distance_function)
    return max([k_distance_value, distance_function(instance1, instance2)])


def local_reachability_density(min_pts, instance, instances, **kwargs):
    """Local reachability density of instance is the inverse of the average reachability
    distance based on the min_pts-nearest neighbors of instance.
    Returns: local reachability density
    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float"""
    (k_distance_value, neighbours) = k_distance(min_pts, instance, instances, **kwargs)
    reachability_distances_array = [0] * len(neighbours)  # n.zeros(len(neighbours))
    for i, neighbour in enumerate(neighbours):
        reachability_distances_array[i] = reachability_distance(min_pts, instance, neighbour, instances, **kwargs)
    sum_reach_dist = sum(reachability_distances_array)
    if sum_reach_dist == 0:
        return float('inf')
    return len(neighbours) / sum_reach_dist


def local_outlier_factor(min_pts, instance, instances, **kwargs):
    """The (local) outlier factor of instance captures the degree to which we call instance an outlier.
    min_pts is a parameter that is specifying a minimum number of instances to consider for computing LOF value.
    Returns: local outlier factor
    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float"""
    (k_distance_value, neighbours) = k_distance(min_pts, instance, instances, **kwargs)
    instance_lrd = local_reachability_density(min_pts, instance, instances, **kwargs)
    lrd_ratios_array = [0] * len(neighbours)
    for i, neighbour in enumerate(neighbours):
        instances_without_instance = set(instances)
        instances_without_instance.discard(neighbour)
        neighbour_lrd = local_reachability_density(min_pts, neighbour, instances_without_instance, **kwargs)
        lrd_ratios_array[i] = neighbour_lrd / instance_lrd
    return sum(lrd_ratios_array) / len(neighbours)


def LOF_outliers(k, instances, **kwargs):
    """Simple procedure to identify outliers in the dataset."""
    instances_value_backup = instances
    outliers = []
    for i, instance in enumerate(instances_value_backup):
        instances = list(instances_value_backup)
        instances.remove(instance)
        l = LOF(instances, **kwargs)
        value = l.local_outlier_factor(k, instance)
        if value > 1:
            outliers.append({"lof": value, "instance": instance, "index": i})
    outliers.sort(key=lambda o: o["lof"], reverse=True)
    return outliers

def ExtractKeyResponses(responses):
    cache=list()
    SensitiveResponses=list()

    for response in responses:
        if response and (response.get("status_code"), response.get("title"), len(str(response.get("text_length"))),str(response.get("text_length"))[:2]) not in cache:
            cache.append(
                    (response.get("status_code"),
                        response.get("title"),
                        len(str(response.get("text_length"))),
                        str(response.get("text_length"))[:2]
                        )
                    )
            SensitiveResponses.append(response)
    '''
    _requests = list()

    for _ in range(0,47):
        for subsite in subsites:
            _requests.append( {
                "webroot": subsite,
                "path": "".join(random.sample("QWETUIOPASDGJKLZXCBNMqwetuiopasdghjklzxvbnm/.?+!@$%&*)_+-=';:",27)) 
                } )
            _requests.append( {
                "webroot": subsite,
                "path": "".join(random.sample("QWETUIOPASDGJKLZXCBNMqwetuiopasdghjklzxvbnm/",7)) 
                } )

    useless_responses = requests_responses(
        _requests,
        IP=True,
        URL=True,
        TEXT=False,
        TITLE=True,
        headers=True,
        STATUS_CODE=True,
        TEXT_LENGTH=True
    )
    ''' 

    '''
    Discretization=False
    TextLength=[ response["text_length"]  for response in responses ]
    if len(TextLength)!=0 and len(TextLength)<700:
        DegreeOfDispersion = Entropy(TextLength)*CoefficientOfVariation(TextLength)
        print(f"[*] Degree of dispersion: {DegreeOfDispersion} ")
        print(f"    Entropy: {Entropy(TextLength)}")
        print(f"    CoefficientOfVariation: {CoefficientOfVariation(TextLength)}")
        if DegreeOfDispersion>=math.e: Discretization=True
    else:
        cache=list()
        Discretization=True
        for response in responses:
            if (response.get("status_code"), response.get("title"), response.get("text_length")) not in cache:
                print(response)
                cache.append(
                        (response.get("status_code"), response.get("title"), response.get("text_length"))
                        )
                SensitiveResponses.append(response)

    if len(responses)>1 and Discretization==False:

        for status_code in list(set([response["status_code"] for response in responses])):
            print(f"\033[34m[*] Extract Key Responses Status-Code={status_code}.\033[0m")

            # LOF_outliers -> list() or [{outlier, index, (instance_x, instance_y)}]
            if all(response.get("title")!=None for response in responses):
                print(responses)
                LOF_results = LOF_outliers(4,
                        [(
                            response["text_length"],
                            len(response["title"])
                            )
                            for response in responses
                            if response and response["status_code"]==status_code
                        ])
            else:
                print(responses)
                LOF_results = LOF_outliers(4,
                        [(
                            response["text_length"],
                            int( "".join(response["ip"].strip().split(".")[:2]) )
                            )
                            for response in responses
                            if response and response["status_code"]==status_code
                        ])

            if not LOF_results: print("    [*] There doesn't seem to be a lof value. ")

            for outlier in LOF_results:
                value=outlier["lof"]
                instance_x, instance_y=outlier["instance"]

                if value>1:
                    SensitiveResponses.append(responses[outlier['index']])
                    print(f"    {responses[outlier['index']]['url']}  Status-code: {status_code}  Instance: {outlier['instance']}  LOF-Value: {value}")
    '''

    return SensitiveResponses
