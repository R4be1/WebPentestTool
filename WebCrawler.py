import os
import re
import time
import json
import datetime
from lib.core.common import *
from lib.core.requestsCore import requests_extracts

class Crawler(object):
    def __init__(self,Semaphore:int=512):
        self.scope=None
        self.deep=int()
        self.max_deep=int()
        self.subdomain=True
        self.Semaphore=Semaphore
        self.tasks_number=int()
        self.filter_file=list()
        self.extractsData=list()
        self.responsesData=list()
        self.filter_directory=list()
        self.media_file_suffix=[".jpg",".png",".mp3",".mp4",".ogg",".wav",".gif",".avi",".webm",".wmv",".mov",".bmp",".jpeg",".pdf",".doc",".docx"]

        #self.requests_extracts_function=ray.put(requests_extracts)
        #{webroot,path,params,data}
 
    def run(self,requestsCache:list) -> list:
        if self.deep>self.max_deep: return self.extractsData
        else: self.deep+=1
        print("\n")

        result_cache=list()
        startRunTime=time.time()
        self.tasks_number+=len(requestsCache)
        
        requestsCache = requests_extracts( requestsCache, self.Semaphore )
        #print(requestsCache)


        NowTime=datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        request_speed="{:.2f}".format( self.tasks_number/(time.time()-startRunTime) )
        print(f"[{NowTime}]  \033[33m{request_speed}req/sec\033[0m  \033[36mRequest-Extract-Amount:{self.tasks_number}\033[0m  TotalTask:{len(requestsCache)}  Deep:{self.deep}")

        requestsCache.sort(key=lambda response: response.get('status_code'))
        for _result in requestsCache:
            if str(_result.get('status_code'))[0]=='3':
                print(f"\033[1;33m[{_result.get('status_code')}]  {_result.get('url')}  {'Text-length:'+_result.get('text_length') if _result.get('text_length') else str()}  Parser-result:{len(_result['Page'])}  {_result['Title']}\033[0m")

            elif str(_result.get('status_code'))[0]=='2':
                print(f"\033[1;32m[{_result.get('status_code')}]  {_result.get('url')}  {'Text-length:'+_result.get('text_length') if _result.get('text_length') else str()}  Parser-result:{len(_result['Page'])}  {_result['Title']}\033[0m")

            elif str(_result.get('status_code'))[0]=='4':
                print(f"\033[1;34m[{_result.get('status_code')}]  {_result.get('url')}  {'Text-length:'+_result.get('text_length') if _result.get('text_length') else str()}  Parser-result:{len(_result['Page'])}  {_result['Title']}\033[0m")

            elif str(_result.get('status_code'))[0]=='5':
                print(f"\033[1;32m[{_result.get('status_code')}]  {_result.get('url')}  {'Text-length:'+_result.get('text_length') if _result.get('text_length') else str()}  Parser-result:{len(_result['Page'])}  {_result['Title']}\033[0m")

            else:
                print(f"\033[1m[{_result.get('status_code')}]  {_result.get('url')}  {'Text-length:'+_result.get('text_length') if _result.get('text_length') else str()}  Parser-result:{len(_result['Page'])}  {_result['Title']}\033[0m")

            for _parses_result in _result.get("Page"):
                self.extractsData.append(_parses_result)
                if self.RequestFilter(_parses_result):
                    result_cache.append(_parses_result)
        # Save result
        with open(os.path.join("output","extractsResults.txt"), 'w') as file:
            json.dump(self.extractsData, file, indent=4)

        if result_cache: self.run(result_cache)

    def init(self,tasks):
        requestsCache=list()
        self.rootDomains=list()

        [requestsCache.append( RequestProcessor(uri) ) for uri in tasks]
        [self.rootDomains.append( getRootDomain( uri ) ) for uri in tasks]
        return requestsCache

    def RequestFilter(self, request) -> bool:
        if not self.scope==None:
            if not re.match(self.scope,request.get("webroot")): return False

        if self.subdomain==True:
            if not [request for rootDomain in self.rootDomains if rootDomain and rootDomain in request.get("webroot")]: return False
        
        for _ in self.media_file_suffix:
            if request.get("path").endswith(_): return False

        for _ in self.filter_directory:
            if request.get("webroot")==_.get("webroot") and os.path.dirname(request.get("path"))==os.path.dirname(_.get("path")):
                return False
        
        for _ in self.filter_file:
            if request.get("webroot")==_.get("webroot") and os.path.dirname(request.get("path"))==_.get("path"):
                return False
        return True

    def ResponseFilter(self, response) -> bool:
        if response:return True

if __name__=="__main__":
    WebCrawler = Crawler()
    tasks = WebCrawler.init(["https://www.baidu.com/"])
    WebCrawler.max_deep = 2
    WebCrawler.run(tasks)

