import os
import re
import sys
import time
import json
import datetime
from lib.core.common import *
from lib.core.requestsCore import requests_extracts

class Crawler(object):
    def __init__(self,Semaphore:int=512):
        self.scope=None
        self.deep=int()
        self.max_deep=int()
        self.subdomain=True
        self.Semaphore=Semaphore
        self.tasks_number=int()
        self.filter_file=list()
        self.extractsData=list()
        self.responsesData=list()
        self.filter_directory=list()
        self.media_file_suffix=[".jpg",".png",".mp3",".mp4",".ogg",".wav",".gif",".avi",".webm",".wmv",".mov",".bmp",".jpeg",".pdf",".doc",".docx"]

        #self.requests_extracts_function=ray.put(requests_extracts)
        #{webroot,path,params,data}
 
    def run(self,requestsCache:list) -> list:
        if self.deep>self.max_deep: return self.extractsData
        else: self.deep+=1
        print("\n")

        result_cache=list()
        startRunTime=time.time()
        self.tasks_number+=len(requestsCache)
        
        requestsCache = requests_extracts( requestsCache, self.Semaphore )
        #print(requestsCache)


        NowTime=datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        request_speed="{:.2f}".format( self.tasks_number/(time.time()-startRunTime) )
        print(f"[{NowTime}]  \033[33m{request_speed}req/sec\033[0m  \033[36mRequest-Extract-Amount:{self.tasks_number}\033[0m  TotalTask:{len(requestsCache)}  Deep:{self.deep}")

        requestsCache.sort(key=lambda response: response.get('status_code'))

        webroot_path_params_temp = list()
        for _result in requestsCache:
            self.responsesData.append(_result)
            if str(_result.get('status_code'))[0]=='3':
                print(f"\033[1;33m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

            elif str(_result.get('status_code'))[0]=='2':
                print(f"\033[1;32m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

            elif str(_result.get('status_code'))[0]=='4':
                print(f"\033[1;34m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

            elif str(_result.get('status_code'))[0]=='5':
                print(f"\033[1;32m[{_result.get('status_code')}]   {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')} {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

            else:
                print(f"\033[1m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

            for _parses_result in _result.get("Page"):
                if (_parses_result not in result_cache) and ((_parses_result.get("webroot")+_parses_result.get("path"), "&".join( sorted(_parses_result.get("params").keys())) ) not in webroot_path_params_temp):
                    if self.RequestFilter(_parses_result):
                        result_cache.append(_parses_result)
                        webroot_path_params_temp.append( (_parses_result.get("webroot")+_parses_result.get("path"), "&".join( sorted(_parses_result.get("params").keys())) ) )
                self.extractsData.append(_parses_result)


        if result_cache: self.run(result_cache)

    def init(self,tasks):
        requestsCache=list()
        self.rootDomains=list()

        [requestsCache.append( RequestProcessor(uri) ) for uri in tasks]
        [self.rootDomains.append( getRootDomain( uri ) ) for uri in tasks]
        return requestsCache

    def RequestFilter(self, request) -> bool:
        if request in self.extractsData:
            return False

        if not self.scope==None:
            if not re.match(self.scope,request.get("webroot")):
                return False

        if self.subdomain==True:
            if not [request for rootDomain in self.rootDomains if rootDomain and rootDomain in request.get("webroot")]:
                return False
        
        for _ in self.media_file_suffix:
            if request.get("path").endswith(_):
                return False

        for _ in self.filter_directory:
            if request.get("webroot")==_.get("webroot") and os.path.dirname(request.get("path"))==os.path.dirname(_.get("path")):
                return False
        
        for _ in self.filter_file:
            if request.get("webroot")==_.get("webroot") and os.path.dirname(request.get("path"))==_.get("path"):
                return False

        return True

    def ResponseFilter(self, response) -> bool:
        if response:return True

if __name__=="__main__":
    WebCrawler = Crawler()
    tasks = WebCrawler.init([sys.argv[1]])
    WebCrawler.max_deep = 10
    WebCrawler.run(tasks)
    print()
    WebCrawler.responsesData.sort(key=lambda response: response.get('status_code'))
    print()

    # Save result
    with open(os.path.join("output","extractsResults.txt"), 'w') as file:
        json.dump(WebCrawler.extractsData, file, indent=4)

    with open(os.path.join("output","responsesResults.txt"), 'w') as file:
        json.dump(WebCrawler.responsesData, file, indent=4)

    for _result in WebCrawler.responsesData:
        if str(_result.get('status_code'))[0]=='3':
            print(f"\033[1;33m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

        elif str(_result.get('status_code'))[0]=='2':
            print(f"\033[1;32m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

        elif str(_result.get('status_code'))[0]=='4':
            print(f"\033[1;34m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

        elif str(_result.get('status_code'))[0]=='5':
            print(f"\033[1;32m[{_result.get('status_code')}]   {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')} {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")

        else:
            print(f"\033[1m[{_result.get('status_code')}]  {str(_result.get('text_length')).ljust(6, ' ') if _result.get('text_length') else str().ljust(6, ' ')}  {_result.get('url').ljust(100,' ')}\033[0m  Parser-result:{len(_result['Page'])}  {_result['Title'].strip()}")
